{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "<div><img src=\"https://www.ibm.com/blogs/bluemix/wp-content/uploads/2017/02/NLU.png\", width=270, height=270, align = 'right'> \n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/IBM_logo.svg/640px-IBM_logo.svg.png\", width = 90, height = 90, align = 'right', style=\"margin:0px 25px\"></div>\n\n# Extract Insights from Social Media with Watson Developer Cloud and Data Science Experience\n\n___________\n\n## Table of Contents\n\n1.  [Load Required Libraries](#loadlibraries)\n2.  [Load Data from DashDB](#loaddata)\n3.  [Exploratory Data Analysis](#exploredata)\n4.  [Take a Sample of Data](#takesample)\n5.  [Read Credentials for NLU, Personality Insights, and Twitter](#getcredentials)\n6.  [Enrich Data with Watson NLU](#enrichnlu)\n7.  [Visualizing Sentiment and Keywords](#sentiment)\n8.  [Enrich Data with Watson Personality Insights](#enrichpi)\n9.  [Spark Machine Learning for User Segmentation](#sparkml)\n10. [Visualizing User Segmentation](#clusters)", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "__________________\n\n<a id=\"loadlibraries\"> </a>\n### Step 1: Load required libraries\n\n**watson-developer-cloud** is the Python SDK for Watson Developer Cloud services: https://github.com/watson-developer-cloud/python-sdk\n\n**tweepy** is a Python library for accessing Twitter API: http://www.tweepy.org/\n\n**wordcloud** is a Python library for generating Word Clouds: https://github.com/amueller/word_cloud/\n\n**plotly** is a Python library for making plots and charts: https://pypi.python.org/pypi/plotly", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "source": "!pip install --upgrade watson-developer-cloud\n!pip install tweepy==3.3.0\n!pip install plotly --upgrade\n!pip install wordcloud", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "________________\n\n<a id=\"loaddata\"> </a>\n## Step 2: Load data from dashDB\n\nFirst step is to load the data. This notebook assumes you have tweets already in a dashDB database. For more details on how to collect relevant tweets into a dashDB database, check out [this](https://github.com/joe4k/twitterstreams) github repository.\n\nIf you're familiar with how creating data connections, you can skip this step.\n\n1. Click on **Data Services** tab and select **Connections**.\n2. Click **Create new connection** (+ sign)\n3. Provide a connection name (dashdbsoftdrinktweets) and select **Data Service** for **Service Category**.\n4. Select the dashDB service instance where the tweets are stored.\n5. Select **BLUDB** database and press **Create**.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Next you need to make the Data Connection we created accessible to your project. \n* Navigate to your project.\n* Click the **Find and Add Data** icon (top right).\n* Click the **Connections** tab.\n* Select the `dashdbsoftdrinktweets` connection.\n* Click **Apply** ==> This makes that data connection accessible to your project.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Lastly, you need to add code to your notebook to read the data from dashDB.\n* Create a new cell in your notebook.\n* Click the **Find and Add Data** icon.\n* Click the **Connections** tab.\n* Find `dashdbsoftdrinktweets` connection and click on **Insert to code** under `dashdbsoftdrinktweets` connection.\nThis adds code into the notebook to load data from dashDB database. \n\nNote that you would select a specific table to load. In our case, we load **DSX_CLOUDANT_SOFTDRINK_TWEETS**.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# copy data into sodaTweetsDF dataframe for processing\nsodaTweetsDF = data_df_1", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "___________\n\n<a id=\"exploredata\"> </a>\n## Step 3: Exploratory Data Analysis", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "source": "# Return top 2 rows of Spark DataFrame\nsodaTweetsDF.limit(2).toPandas()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# Print the schema of the loaded data\nsodaTweetsDF.printSchema()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Drop unneeded columns\nsodaTweetsDF = sodaTweetsDF.drop('_ID','_REV')", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Extract day from the CREATED_AT field. This is useful to plot tweet trends over time.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "import datetime\nfrom datetime import date\nfrom dateutil import parser\n\ndef getDay(date):\n    print 'input date: ', date\n    day = parser.parse(str(date))\n    day = day.date()\n    return day\n\n# Add a field for the day the tweet was created (ignoring hour/minute/second)\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DateType\n\nudfGetDay = udf(getDay, DateType())\n\nsodaTweetsDF = sodaTweetsDF.withColumn('DAY',udfGetDay('CREATED_AT'))\n\n# Verify added field is as expected\nsodaTweetsDF.select(\"DAY\").limit(5).toPandas()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "____________\n\n<a id=\"takesample\"> </a>\n## Step 4: Take a Sample\nFor purposes of this tutorial, we will work with a small sample of the data. In practice, you want to use large data sets for our analysis to capture as many insights as we can. However, to illustrate the approach, we can work with a small dataset.\n\nFurthermore, we want to restrict the number of API calls to the free plan of [Watson Developer Cloud](https://www.ibm.com/watson/developercloud/) services so users can run through the notebook successfully.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Take a sample of the data\n## Limit to 1000 records as Watson NLU allows 1000 free calls per day\nimport random\n\nnum_records = sodaTweetsDF.count()\n#sample_num_records = 950\nsample_num_records = 50\nfraction = float(sample_num_records)/float(num_records)\n\nseed = random.randint(1, 100)\nprint 'Number of records: ', num_records, ' Sample size: ', sample_num_records, ' Fraction: ', fraction, ' Seed: ', seed\nsodaTweetsSampleDF = sodaTweetsDF.sample(False, fraction, seed)\n\nprint 'Number of records to send to NLU:', sodaTweetsSampleDF.count()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Take a sample of the data\n## Limit to 1000 records as Watson NLU allows 1000 free calls per day\n## Use sampleByKey to get equal distribution of samples per day\n\nnum_records = sodaTweetsDF.count()\nsample_num_records = 950\nfraction = float(sample_num_records)/float(num_records)\n\nfractionList = sodaTweetsDF.rdd.map(lambda x: x['DAY']).distinct().map(lambda x: (x,fraction)).collectAsMap()\n\nkeybyday = sodaTweetsDF.rdd.keyBy(lambda x: x['DAY'])\n\n# Returns RDD with length of 2, first col is the key (day) and second col is the original row for the key\n# Take only the actual data (column 1)\nsodaTweetsDFrdd = keybyday.sampleByKey(False,fractionList).map(lambda x: x[1])\nsodaTweetsSampleDF = spark.createDataFrame(sodaTweetsDFrdd,sodaTweetsDF.schema)\n\nprint 'Number of records to send to NLU:', sodaTweetsSampleDF.count()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "tmp = sodaTweetsSampleDF.groupBy('DAY')\\\n                              .agg(F.count('ID')\\\n                              .alias('NUM_TWEETS_PER_DAY'))\ntmp.show()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "____________\n\n<a id=\"getcredentials\"> </a>\n## Step 5: Read Credentials for NLU, Personality Insights, and Twitter\nUpload a json file (for example, sample_creds.json) which has the credentials for NLU, Personality Insights and Twitter to your Object Storage instance.\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "* Click on the Find and Add Data icon \n* Click **Files** tab\n* Either drop your file in the box or click **browse** to select the file with credentials information from your local disk\n* After selecting the file, press **Open** to upload the file to Object Storage.\nHere is the format for the credentials file:\n```\n{\n\t\"nlu_username\": \"YOUR NLU username\",\n\t\"nlu_password\": \"YOUR NLU password\",\n\t\"nlu_version\": \"NLU version\",\n\t\"twitter_consumer_key\": \"YOUR Twitter App consumer key\",\n\t\"twitter_consumer_secret\": \"YOUR Twitter App consumer secret\",\n\t\"twitter_access_token\": \"YOUR Twitter App access token\",\n\t\"twitter_access_token_secret\": \"YOUR Twitter App access token secret\",\n\t\"pi_username\": \"YOUR Personality Insights username\",\n\t\"pi_password\": \"YOUR Personality Insights password\",\n\t\"pi_version\": \"Personality Insights version\"\n}\n```\n* Once uploaded, you should see the sample_creds.json file under Files tab under Find and Add Data column.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Next, read the sample_creds.json credentials file and set the credentials for NLU, Personality Insights, and Twitter.\n* Create a new cell in your notebook.\n* If not open, click the **Find and Add Data** icon.\n* Click the **Files** tab.\n* Find the ```sample_creds.json``` file and click **Insert to code** under the file name \n* Choose **Insert Credentials** ==> this inserts code into your notebook for setting the credentials to read that file from Object Storage.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "from io import BytesIO  \nimport requests  \nimport json \n\ndef get_data(credentials):  \n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage V3.\"\"\"\n\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', credentials['filename']])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.get(url=url2, headers=headers2)\n    return json.loads(resp2.content)\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# Note that we need to reference the credentials object returned by Insert to code (in this example, it is credentials_2)\ncredentials_json = get_data(credentials_4)\nprint 'Credentials for NLU'\nprint 'NLU username: ', credentials_json['nlu_username']\nprint 'NLU password: ', credentials_json['nlu_password']\nprint 'NLU version: ', credentials_json['nlu_version']\n\nprint 'Credentials for Twitter '\nprint 'Twitter consumer key: ', credentials_json['twitter_consumer_key']\nprint 'Twitter consumer secret: ', credentials_json['twitter_consumer_secret']\nprint 'Twitter access token: ', credentials_json['twitter_access_token']\nprint 'Twitter access token secret: ', credentials_json['twitter_access_token_secret']\n\nprint 'Credentials for Personality Insights'\nprint 'Personality Insights username: ', credentials_json['pi_username']\nprint 'Personality Insights password: ', credentials_json['pi_password']\nprint 'Personality Insights version: ', credentials_json['pi_version']", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "__________\n\n<a id=\"enrichnlu\"> </a>\n## Step 6: Enrich Data with  [Watson Natural Language Understanding (NLU)](https://www.ibm.com/watson/developercloud/natural-language-understanding.html) \n\nWatson NLU allows us to extract sentiment and keywords from text.  In our case, the text will be user tweets.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "import watson_developer_cloud\nimport watson_developer_cloud.natural_language_understanding.features.v1 as features\n\n## Define credentials for NLU service\nnlu_username=credentials_json['nlu_username']\nnlu_password=credentials_json['nlu_password']\nnlu_version=credentials_json['nlu_version']\nnlu = watson_developer_cloud.NaturalLanguageUnderstandingV1(version = nlu_version,\n                                                            username = nlu_username,\n                                                            password = nlu_password)\n\n## Send text to NLU and extract Sentiment and Keywords\n## Make sure text is utf-8 encoded\ndef enrichNLU(text):\n    utf8text = text.encode(\"utf-8\")\n    try:\n        result = nlu.analyze(text = utf8text, features = [features.Sentiment(),features.Keywords()])\n        sentiment = result['sentiment']['document']['score']\n        sentiment_label = result['sentiment']['document']['label']\n        keywords = list(result['keywords'])\n    except Exception:\n        result = None\n        sentiment = 0.0\n        sentiment_label = None\n        keywords = None\n    return sentiment, sentiment_label, keywords", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Since we are now working with a smaller data set, we'll convert our dataframe to a Pandas dataframe and enrich the tweets with the results of Watson NLU.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "# Create a Pandas frame and augment with Sentiment analysis and Keywords using Watson NLU\nsodaTweetsSamplePandasDF = sodaTweetsSampleDF.toPandas()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "**This next cell will send the records to the Watson API, so be aware of the number of calls that are being made.  If you are on the free plan which is limited to 1000 API calls per day, then you will only be able to run this cell once.  After that the server will respond with a notice indicating you have reached the maximum number of API calls for the day.**  Also, this could take 60+ seconds so some patience is encouraged. ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## This calls the enrichNLU function which accesses the Watson NLU API\nsodaTweetsSamplePandasDF['SENTIMENT'],sodaTweetsSamplePandasDF['SENTIMENT_LABEL'],\\\nsodaTweetsSamplePandasDF['KEYWORDS'] = zip(*sodaTweetsSamplePandasDF['TEXT_CLEAN'].map(enrichNLU))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# view top two records to verify Sentiment and Keywords enrichments are applied as expected\nprint 'Rows x Columns for sodaTweetsSampleDF:', sodaTweetsSamplePandasDF.shape\nsodaTweetsSamplePandasDF[:10] ", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "from pyspark.sql.types import StringType\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import ArrayType\n\nschema = sodaTweetsSampleDF.schema\nschema1 = StructType([\n    StructField(\"relevance\", FloatType(), True),\n    StructField(\"text\", StringType(), True),                            \n])\n\nkeywordschema = StructType.fromJson(schema1.jsonValue())\nadded_fields = [StructField(\"SENTIMENT\", FloatType(), True),StructField(\"SENTIMENT_LABEL\",StringType(),True),\\\n                StructField(\"KEYWORDS\",ArrayType(keywordschema),True)] \n\nnewfields = StructType(schema.fields + added_fields)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Push the enriched data back out to Spark to continue working within the Spark API\n#enrichedSodaTweetsDF = spark.createDataFrame(sodaTweetsSamplePandasDF)\nenrichedSodaTweetsDF = spark.createDataFrame(sodaTweetsSamplePandasDF,newfields)\n\n# Print the schema of the Spark DataFrame to verify we have all the expected fields\nenrichedSodaTweetsDF.printSchema()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "____________\n\n<a id=\"sentiment\"> </a>\n## Step 7: Visualizing Sentiment and Keywords\n\nTweet trends, sentiment, and keywords give a brand manager a view into consumers' perceptions regarding the brand. These insights can be very useful to the brand manager. \n\nIn this step we will visualize some of the data we received from NLU.  Let's look at tweet trends and sentiment for the **Coke** and **Pepsi** brands.  In addition, let's show the main keywords tweeted for both brands.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Separate tweets by brand\n# To do so, check if the tweet includes which brand and add a column to represent that\nbrandList =['coke','pepsi']\ndef addBrand(text):\n    for brand in brandList:\n        if brand in text.lower():\n            return brand\n    return None", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nudfAddBrand = udf(addBrand, StringType())\n\n# For purposes of separating tweets by brand, we need to run it against original TEXT and not the TEXT_CLEAN\n# This is because in several cases, the brand is referenced with a handle\nenrichedBrandsDF = enrichedSodaTweetsDF.withColumn('BRAND',udfAddBrand('TEXT'))\n\n# view top records to verify brand column extracted as expected\nenrichedBrandsDF.limit(2).toPandas()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "\nTo visualize the difference in sentiment between brands, we'll create a separate dataframe for each brand.\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "from pyspark.sql.functions import col\n\n## Create one DF for Coke, one for Pepsi\ncokeTweetsDF = enrichedBrandsDF.where(col('BRAND') == 'coke')\npepsiTweetsDF = enrichedBrandsDF.where(col('BRAND') == 'pepsi')", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "cokeTweetsDF.head(2)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Now, find the number of tweets with different sentiment labels (Positive, Negative, Neutral) for both Coke and Pepsi.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "from pyspark.sql import functions as F\nfrom pyspark.sql.functions import col\n\n## First for Coke\ncokeTweetsDF = cokeTweetsDF.where(col('SENTIMENT_LABEL').isNotNull())\ncokeSentimentDF = cokeTweetsDF.groupBy('SENTIMENT_LABEL')\\\n                              .agg(F.count('ID')\\\n                              .alias('NUM_TWEETS'))\n\n## Take a look\ncokeSentimentDF.show()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Now for Pepsi\npepsiTweetsDF = pepsiTweetsDF.where(col('SENTIMENT_LABEL').isNotNull())\npepsiSentimentDF = pepsiTweetsDF.groupBy('SENTIMENT_LABEL')\\\n                                .agg(F.count('ID')\\\n                                .alias('NUM_TWEETS'))\n\npepsiSentimentDF.show()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "#### Compare Sentiment Between Coke and Pepsi", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Pull aggregated back to driver and convert to Pandas dataframe for plotting\ncokeSentimentDF = cokeSentimentDF.toPandas()\npepsiSentimentDF = pepsiSentimentDF.toPandas()\n\nimport matplotlib.pyplot as plt\n\n# Plot sentiment\n%matplotlib inline\nplot1_labels = cokeSentimentDF['SENTIMENT_LABEL']\nplot1_values = cokeSentimentDF['NUM_TWEETS']\nplot1_colors = ['green', 'gray', 'red']\n\nplot2_labels = pepsiSentimentDF['SENTIMENT_LABEL']\nplot2_values = pepsiSentimentDF['NUM_TWEETS']\nplot2_colors = ['green', 'gray', 'red']\n\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (23, 10))\naxes[0].pie(plot1_values,  labels = plot1_labels, colors = plot1_colors, autopct = '%1.1f%%')\naxes[0].set_title('Percentage of Sentiment Values in all Coke Tweets')\naxes[0].set_aspect('equal')\naxes[0].legend(loc = \"upper right\", labels=plot1_labels)\n\naxes[1].pie(plot2_values,  labels = plot2_labels, colors = plot2_colors, autopct = '%1.1f%%')\naxes[1].set_title('Percentage of Sentiment Values in all Pepsi Tweets')\naxes[1].set_aspect('equal')\naxes[1].legend(loc = \"upper right\", labels = plot2_labels)\nfig.subplots_adjust(hspace = 1)\nplt.show()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "#### View Sentiment for Coke Over Time", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Get aggregated sentiment for each day\ncokeOverTimeDF = cokeTweetsDF\\\n                    .groupBy('DAY', 'SENTIMENT_LABEL')\\\n                    .agg(F.count('TEXT_CLEAN').alias('NUM_TWEETS'))\\\n                    .orderBy('DAY', ascending = True)\n\n## Get total tweets each day\ncokeTweetsPerDayDF = cokeOverTimeDF.groupBy('DAY')\\\n                        .agg(F.sum('NUM_TWEETS').alias('NUM_TWEETS'))\\\n                        .orderBy('DAY', ascending = True)\n        \n## Convert back to Pandas\ncokeOverTimeDF = cokeOverTimeDF.toPandas()\ncokeTweetsPerDayDF = cokeTweetsPerDayDF.toPandas()\n\n## Identify rows with positive sentiment for each day\npositiveIndex = cokeOverTimeDF['SENTIMENT_LABEL'] == 'positive'\npositiveTweetsDF = cokeOverTimeDF[positiveIndex]\n\n## Identify rows with negative sentiment for each day\nnegativeIndex = cokeOverTimeDF['SENTIMENT_LABEL'] == 'negative'\nnegativeTweetsDF = cokeOverTimeDF[negativeIndex]\n\n## Check results\npositiveTweetsDF[:2]", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "source": "## Define values for matplotlib\nx = cokeTweetsPerDayDF['DAY']\ny = cokeTweetsPerDayDF['NUM_TWEETS']\npy = positiveTweetsDF['NUM_TWEETS']\nny = negativeTweetsDF['NUM_TWEETS']\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(20, 10))\naxes[0].plot(range(len(y)), y, linewidth=2)\naxes[0].set_xticks(x.index.tolist())\naxes[0].set_xticklabels([date.strftime(\"%Y-%m-%d\") for date in x])\naxes[0].margins = 0\naxes[0].set_xlabel('Date/Time')\naxes[0].set_ylabel('Num of Tweets')\naxes[0].set_title('Number of Tweets Over Time - ALL TWEETS')\naxes[0].set_xlim(0, len(y))\naxes[0].legend(loc=\"upper right\", labels=['All Tweets'])\n\naxes[1].plot(range(len(y)), y, linewidth=2, color='blue')\naxes[1].plot(range(len(py)), py, linewidth=2, color='green')\naxes[1].plot(range(len(ny)), ny, linewidth=2, color='red')\naxes[1].set_xticks(x.index.tolist())\naxes[1].set_xticklabels([date for date in x])\naxes[1].margins = 0\naxes[1].set_xlabel('Date/Time')\naxes[1].set_ylabel('Num of Tweets')\naxes[1].set_title('Number of Tweets Over Time - All, Positive and Negative')\naxes[1].set_xlim(0, len(y))\naxes[1].legend(loc=\"upper right\", labels=['All Tweets', 'Positive', 'Negative', 'Undefined Sentiment'])\n\naxes[2].plot(range(len(py)), py, linewidth=2, color='green')\naxes[2].plot(range(len(ny)), ny, linewidth=2, color='red')\naxes[2].set_xticks(x.index.tolist())\naxes[2].set_xticklabels([date for date in x])\naxes[2].margins = 0\naxes[2].set_xlabel('Date')\naxes[2].set_ylabel('Num of Tweets')\naxes[2].set_title('Number of Tweets Over Time - Positive and Negative')\naxes[2].set_xlim(0, len(y))\naxes[2].legend(loc=\"upper right\", labels=['Positive', 'Negative'])\n\n## Rotate x-axes for legibility.\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation = 45)\n\nfig.subplots_adjust(hspace=1)\nplt.show()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Top Keywords\nExtract and disaply top keywords expressed in the tweets referring to Coke and Pepsi.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "from pyspark.sql.functions import explode\n\n# Explode keywords\ncokeKeywordsDF = cokeTweetsDF.select(explode('KEYWORDS').alias('TOPKEYWORDS'))\npepsiKeywordsDF = pepsiTweetsDF.select(explode('KEYWORDS').alias('TOPKEYWORDS'))\n\ncokeTopKeywordsDF = cokeKeywordsDF.select('TOPKEYWORDS').rdd.map(lambda row: row[0]).toDF()\npepsiTopKeywordsDF = pepsiKeywordsDF.select('TOPKEYWORDS').rdd.map(lambda row: row[0]).toDF()\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# UDF to filter profanity words\nprofanityList = ['fuck', 'cunt', 'eh', 'twat', 'shit', 'beep', 'shouty twat']\ndef filter_profanity(word):\n    if word in profanityList:\n        return None\n    if \"http\" in word:\n        return None\n    return word\n\n# UDF to return lower case of word\ndef toLowerCase(word):\n    return word.lower()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# Process extracted keywords to filter profanity and change to lower case\nudfLowerCase = udf(toLowerCase, StringType())\ncokeTopKeywordsDF = cokeTopKeywordsDF.withColumn('TOPKEYWORDS',udfLowerCase('text'))\npepsiTopKeywordsDF = pepsiTopKeywordsDF.withColumn('TOPKEYWORDS',udfLowerCase('text'))\n\nudfFilterProfanity = udf(filter_profanity, StringType())\ncokeTopKeywordsDF = cokeTopKeywordsDF.withColumn('TOPKEYWORDS',udfFilterProfanity('TOPKEYWORDS'))\npepsiTopKeywordsDF = pepsiTopKeywordsDF.withColumn('TOPKEYWORDS',udfFilterProfanity('TOPKEYWORDS'))\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# Group by TOPKEYWORDS and computer average relevance per keyword and also number of tweets for each keyword\ncokeKwdsNumDF = cokeTopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.count('TOPKEYWORDS').alias('KWDSNUMTWEETS'))\npepsiKwdsNumDF = pepsiTopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.count('TOPKEYWORDS').alias('KWDSNUMTWEETS'))\n\ncokeKwdsRelDF = cokeTopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.avg('relevance').alias('KWDSAVGRELEVANCE'))\npepsiKwdsRelDF = pepsiTopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.avg('relevance').alias('KWDSAVGRELEVANCE'))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# join dataframes into one\ncokeTweetsKeywordsDF = cokeKwdsNumDF.join(cokeKwdsRelDF,'TOPKEYWORDS','outer')\npepsiTweetsKeywordsDF = pepsiKwdsNumDF.join(pepsiKwdsRelDF,'TOPKEYWORDS','outer')\n\n# Define keyword score as product of number of tweets expressing that keyword and average relevance\ncokeTweetsKeywordsDF = cokeTweetsKeywordsDF.withColumn('KEYWORD_SCORE',cokeTweetsKeywordsDF.KWDSNUMTWEETS * cokeTweetsKeywordsDF.KWDSAVGRELEVANCE)\npepsiTweetsKeywordsDF = pepsiTweetsKeywordsDF.withColumn('KEYWORD_SCORE',pepsiTweetsKeywordsDF.KWDSNUMTWEETS * pepsiTweetsKeywordsDF.KWDSAVGRELEVANCE)\n\n# Sort dataframe in descending order of KEYWORD_SCORE\ncokeTweetsKeywordsDF = cokeTweetsKeywordsDF.orderBy('KEYWORD_SCORE',ascending=False)\npepsiTweetsKeywordsDF = pepsiTweetsKeywordsDF.orderBy('KEYWORD_SCORE',ascending=False)\n\n# Remove None keywords\ncokeTweetsKeywordsDF = cokeTweetsKeywordsDF.where(col('TOPKEYWORDS').isNotNull())\npepsiTweetsKeywordsDF = pepsiTweetsKeywordsDF.where(col('TOPKEYWORDS').isNotNull())\n\n# Remove coke from coke list and pepsi from pepsi list\n# Note we want to keey pepsi in coke list and coke in pepsi like\ncokeTweetsKeywordsDF = cokeTweetsKeywordsDF.where(col('TOPKEYWORDS') != \"coke\")\npepsiTweetsKeywordsDF = pepsiTweetsKeywordsDF.where(col('TOPKEYWORDS') != \"pepsi\")", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "print \"Coke Top Keywords\"\ncokeTweetsKeywordsDF.show()\n#vv = cokeTopKeywordsDF.groupBy('text').agg(F.count('text') > 1)\n#vv = cokeTopKeywordsDF.where(col('text') == \"kiwi\")\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "print \"Pepsi Top Keywords\"\npepsiTweetsKeywordsDF.show()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "cokeTweetsKeywordsPandas = cokeTweetsKeywordsDF.toPandas()\npepsiTweetsKeywordsPandas = pepsiTweetsKeywordsDF.toPandas()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "from wordcloud import WordCloud\n\n# Process Pandas DataFrame in the right format to leverage wordcloud.py for plotting\n# See documentation: https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py \ndef prepForWordCloud(pandasDF,n):\n    kwdList = pandasDF['TOPKEYWORDS']\n    sizeList = pandasDF['KEYWORD_SCORE']\n    kwdSize = {}\n    for i in range(n):\n        kwd=kwdList[i]\n        size=sizeList[i]\n        kwdSize[kwd] = size\n    return kwdSize", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "%matplotlib inline\nmaxWords = len(cokeTweetsKeywordsPandas)\nnWords = 20\n\n#Generating wordcloud. Relative scaling value is to adjust the importance of a frequency word.\n#See documentation: https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py\ncokeKwdFreq = prepForWordCloud(cokeTweetsKeywordsPandas,nWords)\ncokeWordCloud = WordCloud(max_words=maxWords,relative_scaling=0,normalize_plurals=False).generate_from_frequencies(cokeKwdFreq)\n\npepsiKwdFreq = prepForWordCloud(pepsiTweetsKeywordsPandas,nWords)\npepsiWordCloud = WordCloud(max_words=nWords,relative_scaling=0,normalize_plurals=False).generate_from_frequencies(pepsiKwdFreq)\n\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (23, 10))\n\n# Set titles for images\nax[0].set_title('Top Keywords for Coke')\nax[1].set_title('Top Keywords for Pepsi')\n\n# Plot word clouds\nax[0].imshow(cokeWordCloud)\nax[1].imshow(pepsiWordCloud)\n\n# turn off axis and ticks\nplt.axis(\"off\")\nax[0].tick_params(axis='both',which='both',bottom='off',top='off',left='off',right='off',\n                 labelbottom='off',labeltop='off',labelleft='off',labelright='off') \n\nax[1].tick_params(axis='both',which='both',bottom='off',top='off',left='off',right='off',\n                 labelbottom='off',labeltop='off',labelleft='off',labelright='off') \n\nplt.show()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "_____________\n\n<a id=\"enrichpi\"> </a>\n## Step 8: Enrich Data with [Watson Personality Insights](https://www.ibm.com/watson/developercloud/personality-insights.html)\n\nIn this tutorial, we will create personality profiles for a sample of 100 users as that is the limit of what we can run with free plan for Watson [Personality Insights](https://www.ibm.com/watson/developercloud/personality-insights.html) (PI). \n\nIn practice, you can run personality profiles for all users, or you can choose to run personality profiles for only a select subset of users; maybe ones with most negative sentiment tweets or maybe ones with largest number of followers or posts.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Aggregate users by sentiment\ncokeUserSentimentDF = cokeTweetsDF\\\n                        .groupBy('USER_SCREEN_NAME', 'SENTIMENT_LABEL','SENTIMENT',\\\n                                 'USER_FAVOURITES_COUNT','USER_STATUSES_COUNT',\n                                 'USER_FOLLOWERS_COUNT','USER_FRIENDS_COUNT')\\\n                        .count()\\\n                        .orderBy('SENTIMENT', ascending = True)\n\n## Get negative and positive tweeting users\nnegativeTweetersDF = cokeUserSentimentDF.where(col('SENTIMENT_LABEL') == \"negative\")\npositiveTweetersDF = cokeUserSentimentDF.where(col('SENTIMENT_LABEL') == \"positive\")  \n\n# Take a random sample of 100 users\nnum_users = cokeUserSentimentDF.count()\nsample_num_users = 95\nusrfraction = float(sample_num_users)/float(num_users)\nusrseed = random.randint(1, 100)\n\n## Start off by finding the number of unique users tweeting about Coke\nprint 'Number of unique users tweeting about Coke: ', len(cokeTweetsDF.select('USER_SCREEN_NAME').distinct().collect())\nprint 'Number of negative tweeting folks: ', negativeTweetersDF.count()\nprint 'Number of positive tweeting folks: ', positiveTweetersDF.count()\nprint 'Sample size: ', sample_num_users, ' Fraction: ', usrfraction, ' Seed: ', usrseed\n\nsodaUserSampleDF = cokeUserSentimentDF.sample(False, usrfraction, usrseed)\n\nprint 'Records in sodaUserSampleDF: ', sodaUserSampleDF.count()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "In this step, we collect enough tweets for each unique user and run those tweets through Personality Insights to extract the Big 5 personality traits, also knows as OCEAN (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism).", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Now we'll define several functions to programmatically paint a portrait of each user and their personality.\n\n1. `getTweets()` - to collect tweets from a given user ID\n2. `getPersonality()` - to call Personality Insights on the users' tweets\n3. `extractOCEANtraits()` - to gather the OCEAN scores for each users PI data\n4. `getPItraits()` - to put it all together and return a list of traits for each user", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "## Collect tweets for a given user\ndef getTweets(username):\n    twitter_id = username    \n    try:\n        tweet_collection = api.user_timeline(screen_name = twitter_id, count = 100, include_rts = True)\n        i = 0\n        tweets = []\n        for status in tweet_collection:\n            i = i+1\n            tweets.append(status.text)\n    except Exception:\n        tweets = None\n    return tweets\n\n## Call Personality Insights on the tweets for the user\ndef getPersonality(tweets):\n    # get tweets by user\n    if tweets == None:\n        profile = None\n    else:\n        tweets_content = ' '.join(tweets)\n        # UTF-8 encoding\n        twt = tweets_content.encode('utf-8')\n        # call PI to get personality profile\n        try:\n            profile = personality_insights.profile(twt, content_type = 'text/plain', content_language = None,\n                                           accept ='application/json', accept_language = None, raw_scores = False,\n                                           consumption_preferences = False, csv_headers = False)\n        except Exception:\n            profile = None\n    \n    return profile\n\n\n## Extract OCEAN percentiles from PI data\ndef extractOCEANtraits(profile):\n    if profile == None:\n        openness = None\n        conscientiousness = None\n        extraversion = None\n        agreeableness = None\n        neuroticism = None\n    else:\n        personality = profile['personality']\n        openness = personality[0]['percentile']\n        conscientiousness = personality[1]['percentile']\n        extraversion = personality[2]['percentile']\n        agreeableness = personality[3]['percentile']\n        neuroticism = personality[4]['percentile']\n    \n    return openness, conscientiousness, extraversion, agreeableness, neuroticism\n\n## Combine function calls for a user\ndef getPItraits(user, verbose = F):\n    # get tweets by user\n    if verbose == F:\n        try:\n            tweets = getTweets(user)\n            # run PI profile on extracted tweets\n            profile = getPersonality(tweets)\n            # extract OCEAN traits\n            openness, conscientiousness, extraversion, agreeableness, neuroticism = extractOCEANtraits(profile)\n        except Exception:\n            return None\n        return openness, conscientiousness, extraversion, agreeableness, neuroticism\n    else:\n        print 'Getting tweets for user: ', user\n        try:\n            tweets = getTweets(user)\n            # run PI profile on extracted tweets\n            profile = getPersonality(tweets)\n            # extract OCEAN traits\n            openness, conscientiousness, extraversion, agreeableness, neuroticism = extractOCEANtraits(profile)\n        except Exception:\n            return None\n        return openness, conscientiousness, extraversion, agreeableness, neuroticism", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Now we'll extract Personality Insights traits for the users sampled in step 7, then push the data back to Spark for machine learning.  **This cell will also send data to the API, so be mindful of the number of calls you are making and be patient for the results.**", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Convert to Pandas to use the Watson PI API\nsodaUserSampleDF = sodaUserSampleDF.toPandas()\nsodaUserSampleDF['OPENNESS'], sodaUserSampleDF['CONSCIENTIOUSNESS'],\\\nsodaUserSampleDF['EXTRAVERSION'], sodaUserSampleDF['AGREEABLENESS'],\\\nsodaUserSampleDF['NEUROTICISM'] = zip(*sodaUserSampleDF['USER_SCREEN_NAME'].map(getPItraits))\n\n# Conver back to Spark Dataframe from Pandas dataframe\nuserPersonalityDF = spark.createDataFrame(sodaUserSampleDF)\n\n## Drop rows without any PI enrichment\nuserPersonalityDF = userPersonalityDF.na.drop()\n\n## Check row count and schema\nprint 'Number of records in userPersonalityInsightsDF: ', userPersonalityDF.count()\nuserPersonalityDF.printSchema()\n\nuserPersonalityDF.limit(2).toPandas()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "________________\n\n<a id=\"sparkml\"> </a>\n## Step 9: Spark Machine Learning for User Segmentation \n[Spark MLlib](https://spark.apache.org/docs/latest/ml-guide.html) includes a rich set of machine learning algorithms that are very powerful in extracting insights from data. It is quite typical to need to process the data into the right format before being able to apply these machine learning algorithms. In this step, we apply several steps for processing the data to get it ready for running [Kmeans](https://spark.apache.org/docs/latest/mllib-clustering.html#k-means) clustering algorithm including normalizing **USER_FOLLOWERS_COUNT** and **USER_STATUSES_COUNT** fields as well as extracting the relevant feature set into a Vector to be used for clustering.\n\nWe run Kmeans clustering using two different feature sets.\n* **FeatureSet 1** (no Personality Traits): (SENTIMENT, USER_FOLLOWERS_COUNT, USER_STATUSES_COUNT)\n* **FeatureSet 2** (with Personality Traits): (SENTIMENT, USER_FOLLOWERS_COUNT, USER_STATUSES_COUNT, OPENNESS, CONSCIENTIOUSNESS, EXTRAVERSION, AGREEABLENESS, NEUROTICISM)\n\nFirst, we'll convert the follower and statuses counts to a vector as per the Spark documentation.  Then we can run [MinMaxScaler](https://spark.apache.org/docs/2.1.0/ml-features.html#minmaxscaler) and normalize the counts to a range between 0 and 1.  These are necessary preprocessing steps for the clustering algorithm to work properly.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.linalg import Vectors\n\n## Convert back to Spark Dataframe from Pandas dataframe\nuserPersonalityDF = spark.createDataFrame(sodaUserSampleDF)\n\n## Drop rows without any PI enrichment\nuserPersonalityDF = userPersonalityDF.na.drop()\n\n## Define columns to be converted to vectors\nfollowersVector = VectorAssembler(\n  inputCols=[\"USER_FOLLOWERS_COUNT\"], outputCol=\"USER_FOLLOWERS_COUNT_VECTOR\")\n\nstatusesVector = VectorAssembler(\n  inputCols=[\"USER_STATUSES_COUNT\"], outputCol=\"USER_STATUSES_COUNT_VECTOR\")\n\n## Define our input and output columns for MinMaxScaler\nfollowersScaler = MinMaxScaler(inputCol=\"USER_FOLLOWERS_COUNT_VECTOR\", outputCol=\"USER_FOLLOWERS_COUNT_SCALED\")\nstatusesScaler = MinMaxScaler(inputCol=\"USER_STATUSES_COUNT_VECTOR\", outputCol=\"USER_STATUSES_COUNT_SCALED\")\n\n## Invoke the VectorAssembler transformations and select desired columns\nuserPersonalityDF = followersVector.transform(userPersonalityDF)\nuserPersonalityDF = statusesVector.transform(userPersonalityDF)\n\n## Fit MinMaxScalerModel on our vectors and rescale\nfollowersScalerModel = followersScaler.fit(userPersonalityDF)\nstatusesScalerModel = statusesScaler.fit(userPersonalityDF)\n\nuserPersonalityDF = followersScalerModel.transform(userPersonalityDF)\nuserPersonalityDF = statusesScalerModel.transform(userPersonalityDF)\n\n## User-defined function to convert from vector back to float\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import lit, udf\n\ndef ith_(v, i):\n    try:\n        return float(v[i])\n    except ValueError:\n        return None\n\nudfVecToFloat = udf(ith_, DoubleType())\n\n## Add column for floating point scaled followers\nuserPersonalityDF = userPersonalityDF.withColumn(\"SCALED_FOLLOWERS\", udfVecToFloat('USER_FOLLOWERS_COUNT_SCALED', lit(0)))\\\n                                     .withColumn(\"SCALED_STATUSES\", udfVecToFloat('USER_STATUSES_COUNT_SCALED', lit(0)))\\\n                                     .select(\"USER_SCREEN_NAME\", \"SENTIMENT_LABEL\", \"SENTIMENT\", \"SCALED_FOLLOWERS\", \\\n                                            \"SCALED_STATUSES\", \"OPENNESS\", \"CONSCIENTIOUSNESS\", \"EXTRAVERSION\", \\\n                                            \"AGREEABLENESS\", \"NEUROTICISM\")\n        \n## Lastly, we'll have to add columns for features both with and without Personality Insights\nassemblerWithPI = VectorAssembler(\n    inputCols = ['SENTIMENT','SCALED_FOLLOWERS','SCALED_STATUSES','OPENNESS','CONSCIENTIOUSNESS','EXTRAVERSION',\\\n               'AGREEABLENESS','NEUROTICISM'],\n    outputCol = \"PI_ENRICHED_FEATURES\")\n\nassemblerWithoutPI = VectorAssembler(\n    inputCols = ['SENTIMENT', 'SCALED_FOLLOWERS', 'SCALED_STATUSES'],\n    outputCol = \"BASE_FEATURES\")\n\n\nuserPersonalityDF = assemblerWithPI.transform(userPersonalityDF)\nuserPersonalityDF = assemblerWithoutPI.transform(userPersonalityDF)\n\n## View transformed DF\nuserPersonalityDF.limit(5).toPandas()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "#### Kmeans Clustering \n\nNow that we have prepared the data, we can run Kmeans to assign cluster labels to each user.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "from pyspark.ml.clustering import KMeans\n\n## Define model parameters and set the seed\nbaseKMeans = KMeans(featuresCol = \"BASE_FEATURES\", predictionCol = \"BASE_PREDICTIONS\").setK(5).setSeed(206)\npiKMeans = KMeans(featuresCol = \"PI_ENRICHED_FEATURES\", predictionCol = \"PI_PREDICTIONS\").setK(5).setSeed(206)\n\n## Fit model on our feature vectors\nbaseClustersFit = baseKMeans.fit(userPersonalityDF.select(\"BASE_FEATURES\"))\nenrichedClustersFit = piKMeans.fit(userPersonalityDF.select(\"PI_ENRICHED_FEATURES\"))\n\n## Get the cluster IDs for each user\nuserPersonalityDF = baseClustersFit.transform(userPersonalityDF)\nuserPersonalityDF = enrichedClustersFit.transform(userPersonalityDF)\n\n## Check our work\nuserPersonalityDF.limit(5).toPandas()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "____________\n\n<a id=\"clusters\"> </a>\n## Step 10: Visualization of User Clusters\n\nOk great - we have successfully clustered our users both with and without enrichment data from Watson Personality Insights.  Now we can visualize users through their cluster identity.\n \nIn this step, we visualize the different user clusters obtained with and without using Personality traits. This visualization illustrates the differences between two clustering approachs and shows how Watson [Personality Insights](https://www.ibm.com/watson/developercloud/personality-insights.html) can help provide finer segmentation of users.\n\nGiven this segmentation, brand managers and marketing teams can craft different messaging to target the various user segments to improve brand adoption in the marketplace.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Aggregate users in each cluster and convert to Pandas for plotting\nbaseClusterAggDF = userPersonalityDF.groupBy('BASE_PREDICTIONS').agg(F.count('USER_SCREEN_NAME').alias('NUM_USERS')).toPandas()\nenrichedClusterAggDF = userPersonalityDF.groupBy('PI_PREDICTIONS').agg(F.count('USER_SCREEN_NAME').alias('NUM_USERS')).toPandas()\n\n%matplotlib inline\n# Code courtesy of \n#pandas_softdrink_tweets_grouped_by_sentiment=df_softdrink_tweets_grouped_by_sentiment.toPandas()\n#pandas_softdrink_tweets_grouped_by_sentiment.count()\nplot1_labels = baseClusterAggDF['BASE_PREDICTIONS']\nplot1_values = baseClusterAggDF['NUM_USERS']\nplot1_colors = ['green', 'gray', 'red', 'blue', 'yellow']\nplot2_labels = enrichedClusterAggDF['PI_PREDICTIONS']\nplot2_values = enrichedClusterAggDF['NUM_USERS']\nplot2_colors = ['green', 'gray', 'red', 'blue', 'yellow']\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(23, 10))\naxes[0].pie(plot1_values,  labels=plot1_labels, colors=plot1_colors, autopct='%1.1f%%')\naxes[0].set_title('Cluster distrubtion without Personality traits')\naxes[0].set_aspect('equal')\naxes[0].legend(loc=\"upper right\", labels=plot1_labels)\naxes[1].pie(plot2_values,  labels=plot2_labels, colors=plot2_colors, autopct='%1.1f%%')\naxes[1].set_title('Cluster distrubtion with Personality traits')\naxes[1].set_aspect('equal')\naxes[1].legend(loc=\"upper right\", labels=plot2_labels)\nfig.subplots_adjust(hspace=1)\nplt.show()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "#### Scatter Plots of Clusters using Principal Components Analysis (PCA)\n\nTypically one would visualize clusters by some plotting some aggregate measure of the data, then filling in the data points with different colors based on cluster ID.  In the absence of aggregate metrics, however, we can use Principal Components Analysis to compress our data set down to two dimensions.  Once we've performed PCA we can then plot the values of the two components on the X and Y axis to form a scatterplot.  \n\nLet's try that now.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "from pyspark.ml.feature import PCA\n\n## Get the first two principal components for features with and without enrichment from Personality Insights\npcaBase = PCA(k = 2, inputCol = \"BASE_FEATURES\", outputCol = \"pcaFeaturesBase\")\npcaEnriched = PCA(k = 2, inputCol = \"PI_ENRICHED_FEATURES\", outputCol = \"pcaFeaturesEnriched\")\n\n## Fit the model to our data\npcaBaseModel = pcaBase.fit(userPersonalityDF)\npcaEnrichedModel = pcaEnriched.fit(userPersonalityDF)\n\n## Transform the data get our principal components\nuserPersonalityDF = pcaBaseModel.transform(userPersonalityDF)\nuserPersonalityDF = pcaEnrichedModel.transform(userPersonalityDF)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "We have the principal components, but before we can plot them we have to convert them from a feature vector back to individual columns.  We'll accomplish this using a lambda function to build a RowRDD, then convert back to a DataFrame.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "from pyspark.sql import Row\n\n## Split the features vector into columns with the rdd-based API, then convert to DF and reorder columns\npcaDF = userPersonalityDF.select(\"pcaFeaturesBase\", \"pcaFeaturesEnriched\", \"USER_SCREEN_NAME\", \"BASE_PREDICTIONS\", \"PI_PREDICTIONS\")\\\n                         .rdd.map(lambda x: Row(**{'PC1_BASE': float(x[0][0]), \n                                                   'PC2_BASE': float(x[0][1]),\n                                                   'PC1_ENRICHED': float(x[1][0]),\n                                                   'PC2_ENRICHED': float(x[1][1]),\n                                                   'USER_SCREEN_NAME': str(x[2]),\n                                                   'BASE_PREDICTIONS': int(x[3]),\n                                                   'PI_PREDICTIONS': int(x[4])}))\\\n                         .toDF()\\\n                         .select(\"USER_SCREEN_NAME\", \"PC1_BASE\", \"PC2_BASE\", \"BASE_PREDICTIONS\",\\\n                                 \"PC1_ENRICHED\", \"PC2_ENRICHED\", \"PI_PREDICTIONS\")\npcaDF.limit(5).toPandas()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Now that the dimensionality of our dataset has been reduced to 2, we can view the components on a typical scatter plot.  Let's see how the clusters look from this perspective.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "import plotly \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\nplotly.offline.init_notebook_mode() \n\npcaDF = pcaDF.toPandas()\n\n## For Base Features\ndata = [go.Scatter(x = pcaDF.PC2_BASE, \n                   y = pcaDF.PC1_BASE, \n                   mode = 'markers',\n                   name = 'BASE_PREDICTIONS',\n                   marker = dict(color = pcaDF.BASE_PREDICTIONS, size = '12'),\n                   text = pcaDF.BASE_PREDICTIONS\n                                 )\n       ]\n\nplotly.offline.iplot(data)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "What is interesting to note about the clustering without Personality Insights enrichment is that it appears to have a very clear stratification from -1 to 1.  These clusters appear to be somewhat intuitively grouped together.  What about when we run the same algorithm on the PI-enriched data?", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## For Enriched Features\ndata = [go.Scatter(x = pcaDF.PC2_ENRICHED, \n                   y = pcaDF.PC1_ENRICHED, \n                   mode = 'markers',\n                   name = 'Clusters with PI',\n                   marker = dict(color = pcaDF.PI_PREDICTIONS, size = '12'),\n                   text = pcaDF.PI_PREDICTIONS\n                                 )\n       ]\n\nplotly.offline.iplot(data)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Not quite the same intuitive results.  Perhaps we should iterate over this and try a different clustering algorithm some time.\n\n**At this point you may be wondering what the utility of all this work might be.**  Well, now that we've enriched the user data with Watson APIs and grouped them into clusters, we can track these clusters over time to see how they respond to various metrics.  This could be purchase history, click patterns, or the response to different ad campaigns.  As we build up this data set we'll be able to classify new users and what group they fall into, resulting in a stronger user segmentation model over time.\n\n_______\n\n## Conclusion\n\nWe've accomplished quite a lot in this notebook, so let's take a moment to review.\n\nFirst we loaded our tweet data from dashDB.  Then we enriched it with several Watson APIs - Natural Language Understanding and Personality Insights.  Using Spark and its elegant API we shaped our data, explored it visually, and prepared it for machine learning.  Our approach was to discover user segmentation by assigning cluster IDs to each data point.  Finally, we plotted the results of our clustering with the aid of principal components analysis.  \n\nWhile more work remains to be done - no one said data science was easy - we have seen how we can seamlessly move between Watson APIs, Spark, and Python using the Data Science Experience. \n\n\n\n_______\n\n\n<div><br><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/IBM_logo.svg/640px-IBM_logo.svg.png\" width = 200 height = 200>\n</div><br>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}